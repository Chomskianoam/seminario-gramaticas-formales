{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers para Gramáticas independientes de contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "Natural Language Toolkit es un conjunto de herramientas para crear programas en python que trabajen con lenguaje natural. nltk.org (la organización a cargo de crear y mantener esta librería) pone a disposición de manera online el [NLTK Book](https://www.nltk.org/book/), su libro especializado en el uso de la librería así como la explicación de conceptos generales de PLN.\n",
    "\n",
    "Para estar al día con los cambios en el código, lo mejor es chequear este libro on-line en vez de se versión editada, que puede traer ejemplos deprecados (\"nbest_parse\" vs. \"parse\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramáticas para NLTK\n",
    "\n",
    "Antes de pasar a los parsers, miremos las gramáticas que vamos a \"parsear\", construidas según lo pide el libro de Bird et al.\n",
    "[LINK A GRAMATICA](CFG.cfg)\n",
    "Notemos que la extensión del archivo es \".cfg\", así le vamos a avisar a nltk que la gramática debe ser entendida como \"Context Free Grammar\".\n",
    "Ahora podemos mirar por adentro que la grámatica cuenta con todos los elementos que, según lo que vimos en la clase, constituyen el formalismo de una CFG:\n",
    "\n",
    "Un axioma: O\n",
    "Símbolos no terminales: SN, PRO, NP, etc.\n",
    "Símbolos terminales: martín, cata, etc.\n",
    "Reglas de reescritura: cada una de las líneas de la gramática, que deben indicar que un elemento a la izquierda del signo -> se debe reescribir como los elementos a la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citando a Benveniste:\n",
    "\"Sea cual fuere la extensión del texto considerado, es preciso segmentarlo primero en porciones cada vez más reducidas hasta los elementos no descomponibles\".\n",
    "\n",
    "Por eso, antes de pasar a los parsers, debemos revisar un concepto: la tokenización, es decir, el proceso de transformar una cadena de caracteres en palabras, ya que el input de nuestros parser va a ser una secuancia de palabras y no la cadena oracional completa.\n",
    "\n",
    "El proceso de tokenización es un proceso complejo que muchas veces se sirve de expresiones regulares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Descent Parser\n",
    "\n",
    "Este parser es de tipo **top-down** (analiza de arriba hacia abajo). Es decir que parte del símbolo de inicio y aplica las reglas de la gramática para obtener los constituyentes inmediatos y armar el árbol hasta llegar a los símbolos terminales. \n",
    "\n",
    "Debe chequear que los símbolos terminales coincidan con la secuencia del input sin haberla visto de antemano. Si no hay coincidencia, tiene que retroceder y buscar diferentes alternativas de parseo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasemos al parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Descent Parser\n",
    "\n",
    "def rd_parser(oracion, gramatica):                  # Definimos una función llamada rd_parser con dos argumentos.\n",
    "    oracion = oracion.lower()                       # Convertimos a minúscula la oración utilizando una función nativa de la cadena de caracteres: lower(). \n",
    "        \n",
    "    if oracion.endswith('.'):                       # Otra función nativa de las strings nos ayuda a chequear si la cadena termina en x argumento.\n",
    "        oracion = re.sub('\\.',' ',oracion)          # En este caso, si la oración termina con un punto, se lo quita utilizando la librería de expresiones regulares \"re\".\n",
    "    else:                                           # Si no termina con un punto, \n",
    "        oracion = oracion                           # toma la oración como estaba originalmente.\n",
    "    lista_palabras = oracion.split()              # Dividimos la oración en palabras tomando como separador el espacio en blanco  con otra función nativa de las strings: split.\n",
    "    print(\"- Esta es la lista de palabras resultante: \", lista_palabras) # Split nos devuelve una lista (ordenada) de strings.\n",
    "      \n",
    "    gramatica = nltk.data.load(gramatica)           # Usamos la función de la sub librería \"data\" que nos permite cargar una gramática para que pueda ser usada luego por el parser.    \n",
    "    rd_parser = nltk.RecursiveDescentParser(gramatica) # Instanciamos la clase del parser que nos da NLTK pasandole un argumento obligatorio: la gramática.\n",
    "    for arbol in rd_parser.parse(lista_palabras):    # Una vez que instanciamos la clase, podemos usar sus funciones mientras le pasemos los argumentos requeridos. En este caso, usamos la función \"parser\" a la que le pasaremos nuestra lista de palabras, y la función nos devolverá cada árbol posible en mi gramática para esa oración.\n",
    "        print(\"- Este es el árbol resultante: \", arbol.draw()) # Imprimimos cada árbol en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para correr el Recursive Descent Parser\n",
    "\n",
    "print('Escribí una oración:')                          # Para que me pida que escriba una oración\n",
    "oracion1 = input()                                     # Para que me abra un campo en el que escriba la oración\n",
    "gramatica = 'gramaticas/CFG.cfg'                       # Indicamos el path a nuestra gramatica\n",
    "rd_parser(oracion1, gramatica)                         # Llamamos a la función que creamos con los dos argumentos que establecimos como obligatorios.\n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del recursive descent parser\n",
    "\n",
    "- 1. La recursión a la izquierda provoca un loop infinito. (SN -> PRO | SN NP)\n",
    "- 2. El parser puede llegar a tomar demasiado tiempo en considerar opciones que mirando la oración ya sabemos que no son posibles. (Fernando fuma)\n",
    "- 3. El movimiento de backtracking borra construcciones de consituyentes que podrían ser útiles para otras partes de la oración. (El cigarrillo fue fumado por la persona)\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Recursive Descent Parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Reduce Parser\n",
    "\n",
    "Este parser, en cambio, es **botom-up**, es decir que parte de la secuencia de palabras que conforman la oración a parsear y busca asignarle una estructura acorde con la gramática. Es decir que busca secuencias de palabras que coincidan con el lado derecho de las producciones de la gramática para reemplazarlas por el símbolo del lado izquierdo.\n",
    "\n",
    "Por ejemplo, si encuentra la secuencia \"fuma\" y en la gramática posee la regla V -> \"fuma\", hará el reemplazo por el símbolo V.\n",
    "\n",
    "Ahora bien, el parser intentará que la subsecuencia más larga posible coincida con los símbolos a la derecha, para ello usa un \"stack\" (una pila, donde se apilan cosas), una especie de memoria temporal donde acumula palabras de una secuencia, una a una, mientras intenta hacerlas coincidir con el lado derecho de una producción. Esta es la acción de \"shift\" (desplazamiento).\n",
    "\n",
    "Una vez que la subsecuencia coincide con una de las producciones, la reemplaza por el símbolo del lado izquierdo. Esta es la acción de \"reduce\" (reducir).\n",
    "\n",
    "\n",
    "El parser aplicará estos pasos hasta alcanzar el símbolo del axioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift Reduce Parser\n",
    "\n",
    "def sr_parser(oracion, gramatica):                   # Definimos una función llamada sr_parser con dos argumentos.\n",
    "    oracion = oracion.lower()\n",
    "    if oracion.endswith('.'):\n",
    "        oracion = re.sub('\\.',' ',oracion)\n",
    "    else:\n",
    "        oracion = oracion\n",
    "    lista_palabras = oracion.split()\n",
    "    gramatica = nltk.data.load(gramatica)\n",
    "    sr_parser = nltk.ShiftReduceParser(gramatica)    # Instanciamos otra clase de parser\n",
    "    for arbol in sr_parser.parse(lista_palabras):\n",
    "        print(\"- Este es el árbol resultante: \", arbol)\n",
    "        #return(arbol)                                # Hacemos un retorno para la función, es decir que la función aquí se va a terminar, lo que nos cortara el loop, pero nos dibujará el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Escribí una oración:')\n",
    "oracion2 = input()\n",
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "sr_parser(oracion2, gramatica)   \n",
    "\n",
    "# Oraciones que acepta la gramática: \n",
    "# Cata/Martín/Julia/Maca/Pablo fuma\n",
    "# Cata/Martín/Julia/Maca/Pablo entregó/envió el/la/un/una plaza/facultad/regalo/globo/tabaco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero qué son esos Warnings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algunas limitaciones del shift reduce parser\n",
    "\n",
    "- 1. Solo puede devolver un árbol posible, aunque la oración sea ambigua y acepte más de una estructura.\n",
    "- 2. En cada acción de reducir, debe seleccionar una, aunque haya más de una posible. Y si la posibilidad de hacer shift o reduce es ambivalente, deberá decidir por una de las dos acciones. Fallas en estas decisiones pueden resultar en una falla del parseo y, al no tener implementada una forma de backtracking, si siguió un camino que fue infructuoso, decidirá que esa oración no tiene solución. (Fernando fuma el cigarrillo en el parque)\n",
    "\n",
    "\n",
    "Veamos todo esto en la demo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Demo del Shift Reduce parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart Parser\n",
    "\n",
    "Los parsers que vimos hasta acá tienen deficiencias, sea en eficiencia o completitud. El chart parser usa dynamic programming para ser más eficiente. Dynamic programming es una técnica para desarrollar algoritmos que tiende a solucionar un problema subdividiendolo en sub problemas. Consiste en guardar la solución a esos sub problemas para poder reusarla cada vez que se la necesita.\n",
    "\n",
    "El chart parser aplica esta técnica. Por ejemplo, construirá el SP \"con el telescopio\" una vez y lo guardará en una tabla. Esta tabla se denomina WFST (tabla de subcadenas bien formadas).\n",
    "\n",
    "Vamos a armar una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wfst(tokens, grammar):    \n",
    "    numtokens = len(tokens)    \n",
    "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]    # Esta forma de escribir un loop se llama \"list comprehension\"\n",
    "    for i in range(numtokens):        \n",
    "        productions = grammar.productions(rhs=tokens[i])        \n",
    "        wfst[i][i+1] = productions[0].lhs()    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_wfst(wfst, tokens, grammar, trace=False):    \n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())    \n",
    "    numtokens = len(tokens)    \n",
    "    for span in range(2, numtokens+1):        \n",
    "        for start in range(numtokens+1-span):           \n",
    "            end = start + span            \n",
    "            for mid in range(start+1, end):                \n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]                \n",
    "                if nt1 and nt2 and (nt1,nt2) in index:                    \n",
    "                    wfst[start][end] = index[(nt1,nt2)]                    \n",
    "                    if trace:                        \n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))    \n",
    "    return wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(wfst, tokens):    \n",
    "    print('\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1, len(wfst))]))    \n",
    "    for i in range(len(wfst)-1):        \n",
    "        print(\"%d   \" % i, end=' ')        \n",
    "        for j in range(1, len(wfst)):            \n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=' ')        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 el\n",
      "1 balcon\n",
      "2 fuma\n",
      "3 en\n",
      "4 el\n",
      "5 balcon\n"
     ]
    }
   ],
   "source": [
    "oracion = \"el balcon fuma en el balcon\".split()\n",
    "for indice in range(len(oracion)):\n",
    "    print(indice, oracion[indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_gramatica = nltk.CFG.fromstring( #Chomsky normal form\n",
    "    \"\"\"O -> SN SV\n",
    "    SP -> P SN\n",
    "    SN -> Det NC | 'fernando'\n",
    "    SV -> V NP | V SP\n",
    "    Det -> 'el'\n",
    "    NC -> 'balcon'\n",
    "    V -> 'fuma'\n",
    "    P -> 'en'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6   \n",
      "0    Det  .    .    .    .    .    \n",
      "1    .    NC   .    .    .    .    \n",
      "2    .    .    V    .    .    .    \n",
      "3    .    .    .    P    .    .    \n",
      "4    .    .    .    .    Det  .    \n",
      "5    .    .    .    .    .    NC   \n"
     ]
    }
   ],
   "source": [
    "wfst0 = init_wfst(oracion, chart_gramatica)\n",
    "display(wfst0, oracion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Det [1]  NC [2] ==> [0]  SN [2]\n",
      "[4] Det [5]  NC [6] ==> [4]  SN [6]\n",
      "[3]   P [4]  SN [6] ==> [3]  SP [6]\n",
      "[2]   V [3]  SP [6] ==> [2]  SV [6]\n",
      "[0]  SN [2]  SV [6] ==> [0]   O [6]\n",
      "\n",
      "WFST 1    2    3    4    5    6   \n",
      "0    Det  SN   .    .    .    O    \n",
      "1    .    NC   .    .    .    .    \n",
      "2    .    .    V    .    .    SV   \n",
      "3    .    .    .    P    .    SP   \n",
      "4    .    .    .    .    Det  SN   \n",
      "5    .    .    .    .    .    NC   \n"
     ]
    }
   ],
   "source": [
    "wfst1 = complete_wfst(wfst0, oracion, chart_gramatica, trace=True)\n",
    "display(wfst1, oracion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el resultado de correr el Chart Parser por una oración con nuestra gramática original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramatica = 'gramaticas/CFG.cfg'\n",
    "gramatica = nltk.data.load(gramatica)\n",
    "print(gramatica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(gramatica)\n",
    "for tree in parser.parse(['fernando', 'fuma']):\n",
    "    #print(tree.draw()) # La notación de punto nos permite acceder a métodos del objeto. Descomenten las lineas con \"print\" y miren qué hace cada método.\n",
    "    #print(tree.flatten()) \n",
    "    #print(tree.productions())\n",
    "    #print(tree.) #Descomenten la línea y usen la tecla \"tab\" para ver qué otros métodos ofrece el objeto.\n",
    "    for st in tree.subtrees():\n",
    "        print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar= (\n",
      "('    ', 'S -> NP VP,')\n",
      "('    ', 'VP -> VP PP,')\n",
      "('    ', 'VP -> V NP,')\n",
      "('    ', 'VP -> V,')\n",
      "('    ', 'NP -> Det N,')\n",
      "('    ', 'NP -> NP PP,')\n",
      "('    ', 'PP -> P NP,')\n",
      "('    ', \"NP -> 'John',\")\n",
      "('    ', \"NP -> 'I',\")\n",
      "('    ', \"Det -> 'the',\")\n",
      "('    ', \"Det -> 'my',\")\n",
      "('    ', \"Det -> 'a',\")\n",
      "('    ', \"N -> 'dog',\")\n",
      "('    ', \"N -> 'cookie',\")\n",
      "('    ', \"N -> 'table',\")\n",
      "('    ', \"N -> 'cake',\")\n",
      "('    ', \"N -> 'fork',\")\n",
      "('    ', \"V -> 'ate',\")\n",
      "('    ', \"V -> 'saw',\")\n",
      "('    ', \"P -> 'on',\")\n",
      "('    ', \"P -> 'under',\")\n",
      "('    ', \"P -> 'with',\")\n",
      ")\n",
      "tokens = ['John', 'ate', 'the', 'cake', 'on', 'the', 'table']\n",
      "Calling \"ChartParserApp(grammar, tokens)\"...\n"
     ]
    }
   ],
   "source": [
    "#Demo para el Chart Parser\n",
    "nltk.app.chartparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLLIP Parser\n",
    "\n",
    "Brown Laboratory for Linguistic Information Processing\n",
    "\n",
    "\n",
    "Bllip parser es un \"reranking parser\", es decir, un parser que va a devolver una serie de posibles árboles para una oración, ordenados según su probabilidad del más probable al menos probable; y una vez que obtuvo los 50 mejores árboles, va a aplicar otra estrategia de ranking para reordenar (rerank) este subset de resultados consiguiendo incluso mayor precisión.\n",
    "\n",
    "La probabilidad de un determinado resultado viene dada por el modelo de lenguaje que el parser usa. El modelo provisto por BLLIP fue entrenado con un corpus de árboles en inglés (El Penn TreeBank) pero sus resultados podrían variar dependiendo de los datos usados en el entrenamiento, así como del método utilizado para entrenar. \n",
    "\n",
    "[Eugene Charniak. \"A maximum-entropy-inspired parser.\" Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference. Association for Computational Linguistics, 2000.](https://aclanthology.org/A00-2018.pdf)\n",
    "\n",
    "\n",
    "[Penn TreeBank](https://catalog.ldc.upenn.edu/LDC99T42)\n",
    "\n",
    "\n",
    "[TreeBank Wikipedia](https://es.wikipedia.org/wiki/TreeBank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user bllipparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bllipparser\n",
    "from bllipparser import RerankingParser                             #Importa el parser\n",
    "from bllipparser.ModelFetcher import download_and_install_model     # Descarga e instala el \"modelo\"\n",
    "\n",
    "model_dir = download_and_install_model('WSJ', 'tmp/models')         #Crea una variable con el \"modelo\"\n",
    "rrp = RerankingParser.from_unified_model_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion2 = \"john runs through the hill\"\n",
    "rrp.simple_parse(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-61.473407804090 -15.666584843061 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-70.113439455262 -18.636868101519 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-68.382454287091 -18.998642332884 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-67.410672470591 -19.181059117030 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-69.418171102276 -20.365130424824 (S1 (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-72.496708134442 -20.572325714804 (S1 (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-69.159051872109 -20.809747309476 (S1 (S (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-72.853753339970 -21.228074880022 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-76.156101753673 -21.335983455203 (S1 (S (NP (NNP john) (NNS runs)) (VP (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-74.662999515759 -21.451079292808 (S1 (NP (NNP john) (VBZ runs) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-74.929389026579 -21.615482629952 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-73.786396171009 -21.755007416103 (S1 (S (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-72.946206104644 -21.779601357204 (S1 (S (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-74.582761842937 -21.844278765456 (S1 (S (NP (NNP john)) (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-77.022485938263 -21.867052858419 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-77.325509068846 -21.878272006251 (S1 (S (NP (NNP john)) (VP (VBZ runs) (VP (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-77.510279033428 -22.035874646592 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (RB through)) (NP (DT the) (NN hill)))))\n",
      "-76.050704121763 -22.074858952565 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-80.773178898354 -22.243750069172 (S1 (S (NP (NNP john)) (VP (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-73.828278984451 -22.323632011278 (S1 (S (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-77.091628358056 -22.581085486455 (S1 (S (NP (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-74.428234892074 -23.064278966830 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (RB through) (NP (DT the) (NN hill))))))\n",
      "-77.425474347380 -23.294309854521 (S1 (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.504011379546 -23.333923804501 (S1 (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-79.055131423491 -23.354453604820 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (IN through)) (NP (DT the) (NN hill)))))\n",
      "-77.705251364669 -23.399005947403 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PP (IN through) (S (NP (DT the) (NN hill)))))))\n",
      "-78.482359919830 -23.469333122878 (S1 (NP (NP (NNP john)) (VP (NNS runs) (PP (IN through) (NP (DT the) (NN hill))))))\n",
      "-80.062712748343 -23.470737010839 (S1 (S (NP (NNP john) (NNS runs)) (VP (IN through) (NP (DT the) (NN hill)))))\n",
      "-78.782866665644 -23.562304384649 (S1 (S (NP (NNP john)) (VP (VBZ runs) (NP (IN through) (DT the) (NN hill)))))\n",
      "-77.491453134259 -23.742847165144 (S1 (S (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.667240569154 -23.834006490657 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (RP through)) (NP (DT the) (NN hill)))))\n",
      "-79.599280191154 -23.890858933720 (S1 (S (NP (NNP john)) (VP (VBZ runs) (PRT (IN through)) (NP (DT the) (NN hill)))))\n",
      "-79.813899587185 -23.899108897287 (S1 (S (NP (NNP john)) (VP (VBZ runs) (NP (RB through) (DT the) (NN hill)))))\n",
      "-78.804575134954 -23.930016088884 (S1 (NP (NP (NNP john)) (VP (VBZ runs)) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-78.855847128776 -24.138990673588 (S1 (S (NP (NNP john)) (VP (NNS runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-81.793699416113 -24.279880275800 (S1 (S (NP (NP (NNP john) (NNS runs)) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-81.231388714607 -24.424882238424 (S1 (S (NP (NNP john)) (VP (VBZ runs) (RB through) (NP (DT the) (NN hill)))))\n",
      "-80.420420607420 -24.584549534796 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (VP (PP (IN through) (NP (DT the) (NN hill)))))))\n",
      "-81.029163050303 -24.604300613090 (S1 (NP (NP (NP (NNP john)) (VP (VBZ runs))) (PP (IN through) (NP (DT the) (NN hill)))))\n",
      "-79.536476229337 -24.655956904420 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-79.108441505987 -24.720113419685 (S1 (S (NP (NNP john)) (VP (VBZ runs) (ADVP (IN through) (NP (DT the) (NN hill))))))\n",
      "-78.869947976295 -24.846633299903 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (RP through) (NP (DT the) (NN hill))))))\n",
      "-81.278607366795 -24.880282555794 (S1 (S (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill)))))\n",
      "-80.486055751547 -24.967117132487 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PRT (RP through)) (NP (DT the) (NN hill)))))\n",
      "-81.835582229555 -25.016086210975 (S1 (S (NP (NP (NNP john) (VBZ runs)) (PP (IN through) (NP (DT the) (NNP hill))))))\n",
      "-81.652190172533 -25.093127824227 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (ADVP (RB through)) (NP (DT the) (NN hill)))))\n",
      "-81.458339498773 -26.164378464222 (S1 (S (NP (NNP john)) (VP (NNS runs) (PP (RB through) (NP (DT the) (NN hill))))))\n",
      "-81.639997084258 -26.222913181684 (S1 (S (NP (NNP john) (NNS runs)) (PP (RP through) (NP (DT the) (NN hill)))))\n",
      "-80.101712897705 -26.272459138080 (S1 (S (NP (NNP john) (NNS runs)) (PP (RB through) (NP (DT the) (NN hill)))))\n",
      "-80.679726153480 -26.747590441617 (S1 (NP (NP (NNP john)) (VP (VBZ runs) (PP (RB through) (NP (DT the) (NN hill))))))\n"
     ]
    }
   ],
   "source": [
    "for parse in rrp.parse(oracion2):\n",
    "    print(parse) #Probar los métodos de parse usando notación de . y \"tab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 'NNP'),\n",
       " ('runs', 'VBZ'),\n",
       " ('through', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hill', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrp.tag(oracion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribí una oración en inglés\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEscribí una oración en inglés\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m oracion4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rrp\u001b[38;5;241m.\u001b[39msimple_parse(oracion4)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "print('Escribí una oración en inglés')\n",
    "oracion4 = input()\n",
    "rrp.simple_parse(oracion4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método tree - árboles con el formato del Penn TreeBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracion3 = \"No one saw him disembark in the unanimous night, no one saw the bamboo canoe sink into the sacred mud, but in a few days there was no one who did not know that the taciturn man came from the South\"\n",
    "structure = rrp.simple_parse(oracion3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = bllipparser.Tree(structure)\n",
    "prettytree = tree.pretty_string()\n",
    "sentenceroot = tree.label\n",
    "sentencespan = tree.span()\n",
    "print(tree)\n",
    "print(prettytree)\n",
    "print(sentenceroot)\n",
    "print(sentencespan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de una gramática con pesos de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramatica_pesos = nltk.parse_pcfg(\"\"\"    \n",
    "    S    -> SN SV              [1.0]    \n",
    "    SV   -> V  SN              [1.0]    \n",
    "    SN   -> Det N              [0.8]    \n",
    "    SN   -> NP                 [0.2]    \n",
    "    NP   -> 'Raúl'             [1.0]    \n",
    "    Det  -> 'el'               [1.0]    \n",
    "    N    -> 'perro'            [0.5]    \n",
    "    N    -> 'gato'             [0.5]    \n",
    "    V    -> 'comió'            [1.0]    \n",
    "    \"\"\")\n",
    "\n",
    "print(gramatica_pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treebank en NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "#nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0] # Wall Street Journal\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
